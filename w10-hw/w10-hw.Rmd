---
title: "Week 10 - Homework"
author: "STAT 420, Summer 2019, Janda - joshlj2"
date: ''
output:
  html_document: 
    toc: yes
  pdf_document: default
urlcolor: cyan
---
***

```{r setup, echo = FALSE, message = FALSE, warning = FALSE}
options(scipen = 1, digits = 4, width = 80, fig.align = "center")
```

## Exercise 1 (Simulating Wald and Likelihood Ratio Tests)

In this exercise we will investigate the distributions of hypothesis tests for logistic regression. For this exercise, we will use the following predictors.

```{r}
sample_size = 150
set.seed(420)
x1 = rnorm(n = sample_size)
x2 = rnorm(n = sample_size)
x3 = rnorm(n = sample_size)
```

Recall that

$$
p({\bf x}) = P[Y = 1 \mid {\bf X} = {\bf x}]
$$

Consider the true model

$$
\log\left(\frac{p({\bf x})}{1 - p({\bf x})}\right) = \beta_0 + \beta_1 x_1
$$

where

- $\beta_0 = 0.4$
- $\beta_1 = -0.35$

**(a)** To investigate the distributions, simulate from this model 2500 times. To do so, calculate 

$$
P[Y = 1 \mid {\bf X} = {\bf x}]
$$ 

for an observation, and then make a random draw from a Bernoulli distribution with that success probability. (Note that a Bernoulli distribution is a Binomial distribution with parameter $n = 1$. There is no direction function in `R` for a Bernoulli distribution.)

Each time, fit the model:

$$
\log\left(\frac{p({\bf x})}{1 - p({\bf x})}\right) = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_3
$$

Store the test statistics for two tests:

- The Wald test for $H_0: \beta_2 = 0$, which we say follows a standard normal distribution for "large" samples
- The likelihood ratio test for $H_0: \beta_2 = \beta_3 = 0$, which we say follows a $\chi^2$ distribution (with some degrees of freedom) for "large" samples
```{r}
test_results = data.frame(
                          wald = rep(0, 2500),
                          lrt = rep(0, 2500)
                          )
```

```{r}
for (i in 1:2500) {
  eta = .4 - .35*x1
  p = 1 / (1 + exp(-eta))
  
  y = rbinom(n = sample_size, size = 1, prob = p)
  mod = glm(y ~ x1 + x2 + x3, family = binomial)
  mod2 = glm(y ~ x1, family = binomial)
  
  test_results$wald[i] = (coef(mod)[3] - 0) / summary(mod)$coefficients[3, 2]
  
  test_results$lrt[i] = anova(mod2, mod, test = "LRT")$Deviance[2]
}
```

**(b)** Plot a histogram of the empirical values for the Wald test statistic. Overlay the density of the true distribution assuming a large sample.
```{r}
library(ggplot2)
ggplot(test_results, aes(x=wald)) +
  geom_histogram(aes(y = ..density..)) + 
  stat_function(fun = dnorm, aes(color = 'wald'), 
                size = 2, args = list(mean = 0, 
                sd =  1)) +
  ggtitle("Wald Test Simulated Test Statistics") + 
  xlab("Wald Test Statistic") + 
  ylab("Density") + theme(plot.title = 
                    element_text(hjust=.5),                            legend.position = "bottom") + 
  scale_color_manual(name="", values = c("wald" = "red"), labels = "Standard Normal Distribution")
```


**(c)** Use the empirical results for the Wald test statistic to estimate the probability of observing a test statistic larger than 1. Also report this probability using the true distribution of the test statistic assuming a large sample.
```{r}
p_wald = length(test_results$wald[test_results$wald > 1]) / nrow(test_results)
p_true = pnorm(1, mean = 0, sd = 1, lower.tail = FALSE)
```

</br>
The proportion of Wald test statistics with a test statistic larger than 1 is `r p_wald`. The true probability of a standard normal distribution with a test gstatistic larger than 1 is `r p_true`. These values are very similar, if not equal within some error tolerance. This helps show that (as well as the histogram above), that the Wald test statistic follows a normal distribution given a large sample.

</br>

**(d)** Plot a histogram of the empirical values for the likelihood ratio test statistic. Overlay the density of the true distribution assuming a large sample.

```{r}
ggplot(test_results, aes(x=lrt)) +
  geom_histogram(aes(y = ..density..), bins = 60) + 
  stat_function(fun = dchisq, aes(color = 'lrt'), 
                size = 2, args = list(df = 2)) +
  ggtitle("Likelihood Ratio Test Simulated Test Statistics") + 
  xlab("LRT Test Statistic") + 
  ylab("Density") + theme(plot.title = 
                    element_text(hjust=.5),                            legend.position = "bottom") + 
  scale_color_manual(name="", values = c("lrt" = "red"), labels = "Chi-square Distribution")
```

**(e)** Use the empirical results for the likelihood ratio test statistic to estimate the probability of observing a test statistic larger than 5. Also report this probability using the true distribution of the test statistic assuming a large sample.
```{r}
p_lrt = length(test_results$lrt[test_results$lrt > 5]) / nrow(test_results)
p_true = pchisq(5, df = 2, lower.tail=FALSE)
```

</br>
The proportion of LRT test statistics greater than 5 is `r p_lrt`. The true probability of a chi-square test statistic being larger than 5 with 2 degrees of freedom is `r p_true`. As we can see in the above plot, the empirical results for the likelihood ratio test statistic follows a chi-square distribution closely. So once again, similar to the wald test, our resultant proportion and true probability do not differ by much (the difference can almost be denoted by error). This shows that the likelihood ratio test statistic follows a chi-square distribution.

</br>

**(f)** Repeat **(a)**-**(e)** but with simulation using a smaller sample size of 10. Based on these results, is this sample size large enough to use the standard normal and $\chi^2$ distributions in this situation? Explain.

```{r}
sample_size = 10
set.seed(420)
x1 = rnorm(n = sample_size)
x2 = rnorm(n = sample_size)
x3 = rnorm(n = sample_size)
```
```{r}
test_results2 = data.frame(
                          wald = rep(0, 2500),
                          lrt = rep(0, 2500)
                          )
```

```{r warning=FALSE}
for (i in 1:2500) {
  eta = .4 - .35*x1
  p = 1 / (1 + exp(-eta))
  
  y = rbinom(n = sample_size, size = 1, prob = p)
  mod = glm(y ~ x1 + x2 + x3, family = binomial)
  mod2 = glm(y ~ x1, family = binomial)
  
  test_results2$wald[i] = (coef(mod)[3] - 0) / summary(mod)$coefficients[3, 2]
  
  test_results2$lrt[i] = anova(mod2, mod, test = "LRT")$Deviance[2]
}
#test_results2 = test_results2[1:2499,]
```
```{r}
ggplot(test_results2, aes(x=wald)) +
  geom_histogram(aes(y = ..density..), bins=60) + 
  stat_function(fun = dnorm, aes(color = 'wald'), 
                size = 2, args = list(mean = 0, 
                sd =  1)) +
  ggtitle("Wald Test Simulated Test Statistics") + 
  xlab("Wald Test Statistic") + 
  ylab("Density") + theme(plot.title = 
                    element_text(hjust=.5),                            legend.position = "bottom") + 
  scale_color_manual(name="", values = c("wald" = "red"), labels = "Standard Normal Distribution")
```
```{r}
p_wald = length(test_results2$wald[test_results2$wald > 1]) / nrow(test_results2)
p_true_wald = pnorm(1, mean = 0, sd = 1, lower.tail = FALSE)
```
```{r}
ggplot(test_results2, aes(x=lrt)) +
  geom_histogram(aes(y = ..density..), bins = 60) + 
  stat_function(fun = dchisq, aes(color = 'lrt'), 
                size = 2, args = list(df = 2)) +
  ggtitle("Likelihood Ratio Test Simulated Test Statistics") + 
  xlab("LRT Test Statistic") + 
  ylab("Density") + theme(plot.title = 
                    element_text(hjust=.5),                            legend.position = "bottom") + 
  scale_color_manual(name="", values = c("lrt" = "red"), labels = "Chi-square Distribution")
```
```{r}
p_lrt = length(test_results2$lrt[test_results2$lrt > 5]) / nrow(test_results2)
p_true_lrt = pchisq(5, df = 2, lower.tail=FALSE)
```
***

</br>
When using a sample size of n=10 to create our models, we can see solely by the plots above that our test statistics do not follow their true distributions. This is due to our model not having a large sample size, which these tests require.

</br>
For the Wald test, we can see that our test statistics do not follow a standard normal distribution. The proportion of Wald test statistics greater than 1 is `r p_wald` while the true p-value of a Wald test statistic being greater than 1 is `r p_true_wald`. These p-values are very different, showing that our simulated test statistics do not follow a standard normal distribution which can be seen from the previous simulation is due to the small sample size.

</br>
For the Likelihood Ratio Test, we can see that our test statistics do not follow a Chi-Square distribution. The proportion of Likelihood Ratio Test statistics greater than 5 is `r p_lrt` while the true p-value of a chi-square test statistic being greater than 5 is `r p_true_lrt`. This difference is large between proportion and the true p-value, showing that our simulated test statistics do not follow a Chi-Square distribution due to our small sample size.

</br>
Overall, I do not believe that this smaller sample size is good enough to use Chi-Square or Standard Normal Distribution statistics due to accuracy concerns. In order to accurately use these tests, we will need to increase our sample size.

</br>

## Exercise 2 (Surviving the Titanic)

For this exercise use the `ptitanic` data from the `rpart.plot` package. (The `rpart.plot` package depends on the `rpart` package.) Use `?rpart.plot::ptitanic` to learn about this dataset. We will use logistic regression to help predict which passengers aboard the [Titanic](https://en.wikipedia.org/wiki/RMS_Titanic) will survive based on various attributes.

```{r, message = FALSE, warning = FALSE}
# install.packages("rpart")
# install.packages("rpart.plot")
library(rpart)
library(rpart.plot)
data("ptitanic")
```

For simplicity, we will remove any observations with missing data. Additionally, we will create a test and train dataset.

```{r}
ptitanic = na.omit(ptitanic)
set.seed(42)
trn_idx = sample(nrow(ptitanic), 300)
ptitanic_trn = ptitanic[trn_idx, ]
ptitanic_tst = ptitanic[-trn_idx, ]
```

**(a)** Consider the model

$$
\log\left(\frac{p({\bf x})}{1 - p({\bf x})}\right) = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_3 + \beta_4 x_4 + \beta_5 x_3x_4
$$

where

$$
p({\bf x}) = P[Y = 1 \mid {\bf X} = {\bf x}]
$$

is the probability that a certain passenger survives given their attributes and

- $x_1$ is a dummy variable that takes the value $1$ if a passenger was 2nd class.
- $x_2$ is a dummy variable that takes the value $1$ if a passenger was 3rd class.
- $x_3$ is a dummy variable that takes the value $1$ if a passenger was male.
- $x_4$ is the age in years of a passenger.

Fit this model to the training data and report its deviance.
```{r}
titanic_mod = glm(survived ~ pclass + sex + age + sex:age, data = ptitanic_trn, family = binomial)
q2a_dev = titanic_mod$deviance
```

</br>
The deviance of this fitted model is `r q2a_dev`.

</br>

**(b)** Use the model fit in **(a)** and an appropriate statistical test to determine if class played a significant role in surviving on the Titanic. Use $\alpha = 0.01$. Report:

- The null hypothesis of the test
- The test statistic of the test
- The p-value of the test
- A statistical decision
- A practical conclusion

```{r}
q2b_mod = glm(survived ~ sex + age + sex:age, data = ptitanic_trn, family = binomial)

q2b_test = anova(q2b_mod, titanic_mod, test = "LRT")
```

</br>
$H_o$: $\beta_{2ndclass}$ = $\beta_{3rdclass}$ = 0. There is no statistical significance in passenger class.
</br>
$H_1$: $\beta_{2ndclass}$ $\neq$ $\beta_{3rdclass}$ $\neq$ 0. There is a statistical significance in passenger class.

</br>
In order to test this hypothesis, I will be using the Likelihood Ratio Test.

</br>
Test Statistic: `r q2b_test$Deviance[2]`

</br>
P-Value: `r q2b_test[2, 5]`

</br>
Statistical Decision: Looking at the P-Value of our test, we can reject the null hypothesis that passenger class does not play a significant role (their coefficients are different than zero), in surviving on the Titanic at $\alpha = .01$. We conclude that there is statistical significance in passenger class.

</br>
Practical Conclusion: Passenger class most definitely played a role in likeliness of survival on the Titanic. Our p-value is almost zero, meaning that passenger was very significant. We can assume that those passengers with more money (meaning 1st or 2nd class), had a higher chance of survival.

</br>

**(c)** Use the model fit in **(a)** and an appropriate statistical test to determine if an interaction between age and sex played a significant role in surviving on the Titanic. Use $\alpha = 0.01$. Report:

- The null hypothesis of the test
- The test statistic of the test
- The p-value of the test
- A statistical decision
- A practical conclusion

```{r}
q2c_teststat = (coef(titanic_mod)[6] - 0) / summary(titanic_mod)$coefficients[6, 2]
q2c_pval = 2*pnorm(q2c_teststat, lower.tail=TRUE)
```

</br>
$H_o$: $\beta_{sex*age}$ = 0
</br>
$H_1$: $\beta_{sex*age} \neq$ 0

</br>
In order to test this hypothesis, I will be utilizing the Wald Test.

</br>
Test Statistic: `r q2c_teststat`

</br>
P-Value: `r q2c_pval`

</br>
Statistical Decision: Looking at the P-Value of our test statistic, we fail to reject the null hypothesis that the coefficient on the interaction variable between age and sex is different from zero at $\alpha = .01$. We cannot say that this interaction variable plays a significant role in determining if a person survived on the Titanic.

</br>
Practical Conclusion: The interaction between sex and age does not play a significant role in whether someone survived or died on the Titanic at our given alpha level. We cannot assume that the interaction between sex and age would increase or decrease odds of survival.

</br>

**(d)** Use the model fit in **(a)** as a classifier that seeks to minimize the misclassification rate. Classify each of the passengers in the test dataset. Report the misclassification rate, the sensitivity, and the specificity of this classifier. (Use survived as the positive class.)
```{r}
make_conf_mat = function(predicted, actual) {
  table(predicted = predicted, actual = actual)
}

get_sens = function(conf_mat) {
  conf_mat[2, 2] / sum(conf_mat[, 2])
}

get_spec =  function(conf_mat) {
  conf_mat[1, 1] / sum(conf_mat[, 1])
}
```

```{r}
q2d_testpred = ifelse(
                      predict(titanic_mod, ptitanic_tst, type = "response") > .50,
                      "survived",
                      "died"
                      )

q2d_misclassification_rate = mean(q2d_testpred != ptitanic_tst$survived)

q2d_confmatr = make_conf_mat(q2d_testpred, ptitanic_tst$survived)

q2d_sensitivity = get_sens(q2d_confmatr)

q2d_specificity = get_spec(q2d_confmatr)

```

</br>
Misclassification Rate: `r q2d_misclassification_rate`

</br>
Sensitivity: `r q2d_sensitivity`

</br>
Specificity: `r q2d_specificity`

</br>

***

## Exercise 3 (Breast Cancer Detection)

For this exercise we will use data found in [`wisc-train.csv`](wisc-train.csv) and [`wisc-test.csv`](wisc-test.csv), which contain train and test data, respectively. `wisc.csv` is provided but not used. This is a modification of the Breast Cancer Wisconsin (Diagnostic) dataset from the UCI Machine Learning Repository. Only the first 10 feature variables have been provided. (And these are all you should use.)

- [UCI Page](https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic))
- [Data Detail](https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.names)

You should consider coercing the response to be a factor variable if it is not stored as one after importing the data.
```{r}
library(readr)
wisc_trn = read_csv('wisc-train.csv')
wisc_trn[, "class"] = lapply(wisc_trn[, "class"], factor)
wisc_tst = read_csv('wisc-test.csv')
wisc_tst[, "class"] = lapply(wisc_tst[, "class"], factor)
```


**(a)** The response variable `class` has two levels: `M` if a tumor is malignant, and `B` if a tumor is benign. Fit three models to the training data.

- An additive model that uses `radius`, `smoothness`, and `texture` as predictors
- An additive model that uses all available predictors
- A model chosen via backwards selection using AIC. Use a model that considers all available predictors as well as their two-way interactions for the start of the search.

For each, obtain a 5-fold cross-validated misclassification rate using the model as a classifier that seeks to minimize the misclassification rate. Based on this, which model is best? Relative to the best, are the other two underfitting or over fitting? Report the test misclassification rate for the model you picked as the best.

```{r warning=FALSE}
set.seed(420)
library(boot)
library(knitr)
q3_addmod_partial = glm(class ~ radius + smoothness + texture, data = wisc_trn, family = binomial)
q3_addmod_full = glm(class ~ ., data = wisc_trn, family = binomial)

q3_twoway_start = glm(class ~ (.)^2, data = wisc_trn, family = binomial)
q3_twoway_end = step(q3_twoway_start, direction = 'backward', trace = 0)
#5 fold CV for each model
mc_addmod_partial = cv.glm(wisc_trn, q3_addmod_partial, K=5)$delta[1]
mc_addmod_full = cv.glm(wisc_trn, q3_addmod_full, K=5)$delta[1]
mc_twoway_end = cv.glm(wisc_trn, q3_twoway_end, K=5)$delta[1]

best_tst_pred = ifelse(
                        predict(q3_twoway_end, wisc_tst, type = "response") > .50,
                          "M",
                          "B"
                      )

best_tst_mc = mean(best_tst_pred != wisc_tst$class)

q3a_results = data.frame(
                          "Partial Additive Model" = mc_addmod_partial,
                          "Full Additive Model" = mc_addmod_full,
                          "End Two-Way Interaction Model" = mc_twoway_end
                      )
kable(t(q3a_results), col.names = "Misclassification Rate")
```

</br>
Looking at the table above of misclassification rates, we can see that the best model is the model selected from backwards selection using AIC. This model has a misclassification rate of `r mc_twoway_end`. In regards to the other models, both additive models seem to be underfitting the data due to containing less than or equal number of coefficients (The models are too simple). The test misclassification rate for this model is `r best_tst_mc`.

</br>


**(b)** In this situation, simply minimizing misclassifications might be a bad goal since false positives and false negatives carry very different consequences. Consider the `M` class as the "positive" label. Consider each of the probabilities stored in `cutoffs` in the creation of a classifier using the **additive** model fit in **(a)**.

```{r}
cutoffs = seq(0.01, 0.99, by = 0.01)

q3b_results = data.frame(
                          sensitivity = rep(0, 99),
                          specificity = rep(0, 99),
                          cutoffs = seq(0.01, 0.99, by = 0.01)
                        )

```
```{r}
i = 1
for (cutoff in cutoffs) {
  predictions = ifelse(
                        predict(q3_addmod_full, wisc_tst, type = "response") > cutoff,
                          "M",
                          "B"
                      )
  misclassification_rate = mean(predictions != wisc_tst$class)
  
  conf_matr = make_conf_mat(predictions, wisc_tst$class)
  q3b_results$sensitivity[i] = get_sens(conf_matr)
  q3b_results$specificity[i] = get_spec(conf_matr)
  
  #increase index..
  i = i + 1
}
```
```{r}
ggplot(q3b_results, aes(x = cutoffs)) + 
  geom_line(data = q3b_results, aes(y = sensitivity, color = "sensitivity"), size = 2) +
  geom_line(data = q3b_results, aes(y = specificity, color = "specificity"), size = 2) +
  labs(x = "Cutoffs", y = "Sensitivity") +
  scale_y_continuous(sec.axis = sec_axis(~.*1, name = "Specificity"))

```

That is, consider each of the values stored in `cutoffs` as $c$. Obtain the sensitivity and specificity in the test set for each of these classifiers. Using a single graphic, plot both sensitivity and specificity as a function of the cutoff used to create the classifier. Based on this plot, which cutoff would you use? (0 and 1 have not been considered for coding simplicity. If you like, you can instead consider these two values.)

$$
\hat{C}(\bf x) = 
\begin{cases} 
      1 & \hat{p}({\bf x}) > c \\
      0 & \hat{p}({\bf x}) \leq c 
\end{cases}
$$

</br>
Based on this plot, I would use a cutoff around .82. This is because for disagnosing whether a tumor if malignant or benign, you will want a high sensitivity. This is become sensitivity is the true positive rate, so when sensitivity is high the false negative rate will be low. For diagnosing whether a tumor is malignant, we want to avoid classifying a tumor as benign when it is truly malignant whenever possible. I chose the cutoff at .82 because this is the intersection point for sensitivity / specificity which both are at around .90. Since this is high for both, it is a good cutoff point and will not be detrimental to any classification.

</br>
[Check out my github!](http://github.com/joshjanda1/stat-420)