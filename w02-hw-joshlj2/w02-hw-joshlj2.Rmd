---
title: "Week 2 - Homework"
author: "STAT 420, Summer 2019, Josh Janda - joshlj2"
date: ''
output:
  pdf_document: default
  html_document: 
    toc: yes
urlcolor: cyan
---

# Directions

- Be sure to remove this section if you use this `.Rmd` file as a template.
- You may leave the questions in your final document.

***

## Exercise 1 (Using `lm`)

For this exercise we will use the `cats` dataset from the `MASS` package. You should use `?cats` to learn about the background of this dataset.

**(a)** Suppose we would like to understand the size of a cat's heart based on the body weight of a cat. Fit a simple linear model in `R` that accomplishes this task. Store the results in a variable called `cat_model`. Output the result of calling `summary()` on `cat_model`.
```{r}
library(MASS)
cat_model = lm(Hwt ~ Bwt, data=cats)
summary(cat_model)
```


**(b)** Output only the estimated regression coefficients. Interpret $\hat{\beta_0}$ and $\beta_1$ in the *context of the problem*. Be aware that only one of those is an estimate.
```{r}
cat_model$coefficients
```
$\hat{\beta_0}$ is the intercept of the model and can be interpreted as the size of a cat's heart if the body weight of a cat is zero.  In numerical terms, if a cat's body weight is equal to zero then we would predict it's heart size to be -.3567. $\beta_1$ is the true population slope coefficient, so in the population it means that the size of a cats heart increases by $\beta_1$ with a one unit increase in body weight. We do not know the true $\beta_1$, so we must estimate it using our sample data.

<br/>

**(c)** Use your model to predict the heart weight of a cat that weights **2.7** kg. Do you feel confident in this prediction? Briefly explain.
```{r}
predict_c = data.frame(Bwt = c(2.7))
predict(cat_model, newdata = predict_c)
cats$Bwt
```
I feel confident in this prediction due to our dataset containing values either exactly 2.7 or close to 2.7 kg. Since our dataset contains this value, it will be able to more accurately predict the heart weight.
<br/>


**(d)** Use your model to predict the heart weight of a cat that weights **4.4** kg. Do you feel confident in this prediction? Briefly explain.
```{r}
predict_d = data.frame(Bwt = c(4.4))
predict(cat_model, newdata = predict_d)
```
I do not feel confident in this prediction since our dataset does not contain any values greater than 3.9. This will make this prediction inaccurate and therefore unreliable and I cannot be confident in it's predicted value.
<br/>


**(e)** Create a scatterplot of the data and add the fitted regression line. Make sure your plot is well labeled and is somewhat visually appealing.
```{r}
library(ggplot2)
ggplot(cats, aes(x = Bwt, y = Hwt)) + geom_point(aes(size = Bwt), color = 'cyan') + ggtitle('Heart Size vs Body Weight in Cats') + xlab('Body Weight (Kg)') + ylab('Heart Size') + theme(plot.title = element_text(hjust=.5)) + geom_smooth(method = lm, formula = y ~ x, aes(color = 'yellow')) + scale_color_manual(values = c('yellow'), labels=c('Fitted Regression Line'), name = 'Legend')

```



**(f)** Report the value of $R^2$ for the model. Do so directly. Do not simply copy and paste the value from the full output in the console after running `summary()` in part **(a)**.
```{r}
#summary(cat_model)$r.squared Do not use summary..
SSR_q1 = sum(resid(cat_model)^2)
SST_q1 = sum((cats$Hwt - mean(cats$Hwt))^2)
1 - (SSR_q1 / SST_q1)
#or
summary(cat_model)$r.squared

```



***

## Exercise 2 (Writing Functions)

This exercise is a continuation of Exercise 1.

**(a)** Write a function called `get_sd_est` that calculates an estimate of $\sigma$ in one of two ways depending on input to the function. The function should take three arguments as input:

- `fitted_vals` - A vector of fitted values from a model
- `actual_vals` - A vector of the true values of the response
- `mle` - A logical (`TRUE` / `FALSE`) variable which defaults to `FALSE`

The function should return a single value:

- $s_e$ if `mle` is set to `FALSE`.
- $\hat{\sigma}$ if `mle` is set to `TRUE`.
```{r}
get_sd_est = function(fitted_vals, actual_vals, mle = FALSE) {
  residuals_func = actual_vals - fitted_vals
  RSS = sum(residuals_func^2)
  if (mle == TRUE) {
    sqrt(RSS / length(residuals_func))
  }else{
    sqrt(RSS / (length(residuals_func) - 2))
  }
}
```


**(b)** Run the function `get_sd_est` on the residuals from the model in Exercise 1, with `mle` set to `FALSE`. Explain the resulting estimate in the context of the model.
```{r}
get_sd_est(cat_model$fitted.values, cats$Hwt, mle = FALSE)
```
This estimate is with mle set to false. The estimate we get is with adjustments to the number of parameters in the model, which is two. We do this since we are building our model from sample data, not a population. Since this is typically how standard error of regression is calculated, this estimate will match the estimate we get from our model summary. We also do this calculation since we do not know the true ${\sigma}$, so we must estimate it using this calculation which gives $s_e$.

**(c)** Run the function `get_sd_est` on the residuals from the model in Exercise 1, with `mle` set to `TRUE`. Explain the resulting estimate in the context of the model. Note that we are trying to estimate the same parameter as in part **(b)**.
```{r}
get_sd_est(cat_model$fitted, cats$Hwt, mle=TRUE)
```
With mle set to true, we are trying to estimate population parameters. In this case, we do not adjust for degrees of freedom (number of parameters in the model). This will give us the estimated value of ${\sigma}$ and not the standard error of regression ($s_e$).

**(d)** To check your work, output `summary(cat_model)$sigma`. It should match at least one of **(b)** or **(c)**.
```{r}
summary(cat_model)$sigma
```
Matches part b, which is consistent with my reasoning in part b.
<br/>

***

## Exercise 3 (Simulating SLR)

Consider the model

\[
Y_i = 5 + -3 x_i + \epsilon_i
\]

with 

\[
\epsilon_i \sim N(\mu = 0, \sigma^2 = 10.24)
\]

where $\beta_0 = 5$ and $\beta_1 = -3$.

This exercise relies heavily on generating random observations. To make this reproducible we will set a seed for the randomization. Alter the following code to make `birthday` store your birthday in the format: `yyyymmdd`. For example, [William Gosset](https://en.wikipedia.org/wiki/William_Sealy_Gosset), better known as *Student*, was born on June 13, 1876, so he would use:

```{r}
birthday = 19981127
set.seed(birthday)
```

**(a)** Use `R` to simulate `n = 25` observations from the above model. For the remainder of this exercise, use the following "known" values of $x$.

```{r}
x = runif(n = 25, 0, 10)
```

You may use [the `sim_slr ` function provided in the text](http://daviddalpiaz.github.io/appliedstats/simple-linear-regression.html#simulating-slr). Store the data frame this function returns in a variable of your choice. Note that this function calls $y$ `response` and $x$ `predictor`.
```{r}
sim_slr = function(x, beta_0, beta_1, sigma_squared) {
  n = length(x)
  epsilon = rnorm(n, mean=0, sd = sqrt(sigma_squared))
  y = beta_0 + beta_1*x + epsilon
  data.frame(predictor = x, response = y)
}
sim_data_q3 = sim_slr(x, beta_0 = 5, beta_1 = -3, sigma_squared = 10.24)
```



**(b)** Fit a model to your simulated data. Report the estimated coefficients. Are they close to what you would expect? Briefly explain.
```{r}
fitted_model_q3 = lm(sim_data_q3$response ~ sim_data_q3$predictor)
fitted_model_q3$coefficients
```


**(c)** Plot the data you simulated in part **(a)**. Add the regression line from part **(b)** as well as the line for the true model. Hint: Keep all plotting commands in the same chunk.
```{r}
library(ggplot2)
ggplot(sim_data_q3, aes(x=predictor, y=response)) + geom_point(color='limegreen') + geom_smooth(method='lm', formula = y~x, aes(color = "deeppink"), se=FALSE, show.legend = TRUE) + geom_abline(aes(slope = -3, intercept = 5, color="orange"), show.legend = TRUE) + labs(x = "Predictor", y = "Response") + scale_color_manual(name = "Key", values = c("deeppink", "orange"), labels = c("Estimated Regression Line", "Actual Line")) + ggtitle("Response vs Predictor") + theme(plot.title = element_text(hjust=.5))
```


**(d)** Use `R` to repeat the process of simulating `n = 25` observations from the above model $1500$ times. Each time fit a SLR model to the data and store the value of $\hat{\beta_1}$ in a variable called `beta_hat_1`. Some hints:

- Consider a `for` loop.
- Create `beta_hat_1` before writing the `for` loop. Make it a vector of length $1500$ where each element is `0`.
- Inside the body of the `for` loop, simulate new $y$ data each time. Use a variable to temporarily store this data together with the known $x$ data as a data frame.
- After simulating the data, use `lm()` to fit a regression. Use a variable to temporarily store this output.
- Use the `coef()` function and `[]` to extract the correct estimated coefficient.
- Use `beta_hat_1[i]` to store in elements of `beta_hat_1`.
- See the notes on [Distribution of a Sample Mean](http://daviddalpiaz.github.io/appliedstats/introduction-to-r.html#distribution-of-a-sample-mean) for some inspiration.

You can do this differently if you like. Use of these hints is not required.
```{r}
beta_hat_1 = rep(0, 1500)
for (i in 1:length(beta_hat_1)-1) {
  temp_data = sim_slr(x, beta_0 = 5, beta_1 = -3, sigma_squared = 10.24)
  temp_model = lm(temp_data$response ~ temp_data$predictor)
  
  beta_hat_1[i] = temp_model$coefficients[[2]]
}
#look at distribution of beta_hat_1.. should be approx normal with mean -3.
#hist(beta_hat_1, breaks=60)
```


**(e)** Report the mean and standard deviation of `beta_hat_1`. Do either of these look familiar?
```{r}
mean(beta_hat_1)
sd(beta_hat_1)
```
The mean of $\hat{\beta}_1$ looks familiar, as it is the true value of $\beta_1$. Since we simulated the value of $\hat{\beta}_1$ 1,500 times the list of values should have a mean very close to the true value of $\beta_1$, as long as our estimates are unbiased. The standard deviation of $\hat{\beta}_1$ looks familiar as well, and should be very close to the standard error of $\hat{\beta}_1$, which looking at the summary of the original model we made confirms this. See below:
```{r}
summary(fitted_model_q3)$coefficients[2,2]
```


**(f)** Plot a histogram of `beta_hat_1`. Comment on the shape of this histogram.
```{r}
beta_hat_1_df = data.frame(betahat1 = beta_hat_1)
ggplot(beta_hat_1_df, aes(x=betahat1)) + geom_histogram(aes(fill='orange'), bins = 60) + xlab('Beta Hat 1 Values') + ggtitle('Histogram of Beta Hat 1') + theme(plot.title = element_text(hjust = 0.5))
```
The shape of the histogram is as expected. The distribution of $\hat{\beta}_1$, when simulated 1,500 times, is centered (mean) around -3, which is the coefficient of beta one. The distribution is also very close to normal, which confirms the central limit theorem. The more beta hat ones you simulate, the more the histogram collapses into center -3. This is shown below:

```{r}
beta_hat_1_extra = rep(0, 10000)
for (i in 1:length(beta_hat_1_extra)-1) {
  temp_data = sim_slr(x, beta_0 = 5, beta_1 = -3, sigma_squared = 10.24)
  temp_model = lm(temp_data$response ~ temp_data$predictor)
  
  beta_hat_1_extra[i] = temp_model$coefficients[[2]]
}

beta_hat_1_extradf = data.frame(betahat1 = beta_hat_1_extra)
ggplot(beta_hat_1_extradf, aes(x=betahat1)) + geom_histogram(aes(fill='orange'), bins = 60) + xlab('Beta Hat 1 Values') + ggtitle('Histogram of Beta Hat 1') + theme(plot.title = element_text(hjust = 0.5))
```


***

## Exercise 4 (Be a Skeptic)

Consider the model

\[
Y_i = 3 + 0 \cdot x_i + \epsilon_i
\]

with

\[
\epsilon_i \sim N(\mu = 0, \sigma^2 = 4)
\]

where $\beta_0 = 3$ and $\beta_1 = 0$.

Before answering the following parts, set a seed value equal to **your** birthday, as was done in the previous exercise.

```{r}
birthday = 19981127
set.seed(birthday)
```

**(a)** Use `R` to repeat the process of simulating `n = 75` observations from the above model $2500$ times. For the remainder of this exercise, use the following "known" values of $x$.

```{r}
x = runif(n = 75, 0, 10)
beta_hat_1 = rep(0, 2500)
for (i in 1:length(beta_hat_1)-1) {
  temp_data = sim_slr(x, beta_0 = 3, beta_1 = 0, sigma_squared = 4)
  temp_model = lm(temp_data$response ~ temp_data$predictor)
  
  beta_hat_1[i] = temp_model$coefficients[[2]]
}

```

Each time fit a SLR model to the data and store the value of $\hat{\beta_1}$ in a variable called `beta_hat_1`. You may use [the `sim_slr ` function provided in the text](http://daviddalpiaz.github.io/appliedstats/simple-linear-regression.html#simulating-slr). Hint: Yes $\beta_1 = 0$.

**(b)** Plot a histogram of `beta_hat_1`. Comment on the shape of this histogram.
```{r}
beta_hat_1_df = data.frame(betahat1 = beta_hat_1)
ggplot(beta_hat_1_df, aes(x=betahat1)) + geom_histogram(aes(fill='orange'), bins = 60) + xlab('Beta Hat 1 Values') + ggtitle('Histogram of Beta Hat 1') + theme(plot.title = element_text(hjust = 0.5))
```


**(c)** Import the data in [`skeptic.csv`](skeptic.csv) and fit a SLR model. The variable names in `skeptic.csv` follow the same convention as those returned by `sim_slr()`. Extract the fitted coefficient for $\beta_1$.
```{r}
library(readr)
skeptic = read.csv('skeptic.csv')
skeptic_mod = lm(skeptic$response ~ skeptic$predictor)
skeptic_beta1 = skeptic_mod$coefficients[[2]]
```



**(d)** Re-plot the histogram from **(b)**. Now add a vertical red line at the value of $\hat{\beta_1}$ in part
**(c)**. To do so, you'll need to use `abline(v = c, col = "red")` where `c` is your value.
```{r}
ggplot(beta_hat_1_df, aes(x=betahat1)) + geom_histogram(aes(fill='orange'), bins = 60) + geom_vline(xintercept = skeptic_beta1, col='blue')
```


**(e)** Your value of $\hat{\beta_1}$ in **(c)** should be negative. What proportion of the `beta_hat_1` values is smaller than your $\hat{\beta_1}$? Return this proportion, as well as this proportion multiplied by `2`.
```{r}
length(beta_hat_1[which(beta_hat_1 < skeptic_beta1)]) / length(beta_hat_1)
length(beta_hat_1[which(beta_hat_1 < skeptic_beta1)]) / length(beta_hat_1) * 2
```



**(f)** Based on your histogram and part **(e)**, do you think the [`s  keptic.csv`](skeptic.csv) data could have been generated by the model given above? Briefly explain.
</br>
I believe that there is a small chance that the skeptic data could have been generated by the model given above. There is a just a very small chance that it could have happened, but there is a non-zero probability as shown above by the proportions. Since the skeptic data is only one set of 75 points, and we generated 2500 sets, it is definitely possible since some of our generated datasets has a beta hat 1 less than the skeptic datasets beta hat 1.

***

## Exercise 5 (Comparing Models)

For this exercise we will use the `Ozone` dataset from the `mlbench` package. You should use `?Ozone` to learn about the background of this dataset. You may need to install the `mlbench` package. If you do so, do not include code to install the package in your `R` Markdown document.

For simplicity, we will perform some data cleaning before proceeding.

```{r}
data(Ozone, package = "mlbench")
Ozone = Ozone[, c(4, 6, 7, 8)]
colnames(Ozone) = c("ozone", "wind", "humidity", "temp")
Ozone = Ozone[complete.cases(Ozone), ]
```

We have:

- Loaded the data from the package
- Subset the data to relevant variables
    - This is not really necessary (or perhaps a good idea) but it makes the next step easier
- Given variables useful names
- Removed any observation with missing values
    - This should be given much more thought in practice

For this exercise we will define the "Root Mean Square Error" of a model as

\[
\text{RMSE} = \sqrt{\frac{1}{n} \sum_{i = 1}^{n}(y_i - \hat{y}_i)^2}.
\]

**(a)** Fit three SLR models, each with "ozone" as the response. For the predictor, use "wind speed," "humidity percentage," and "temperature" respectively. For each, calculate $\text{RMSE}$ and $R^2$. Arrange the results in a markdown table, with a row for each model. Suggestion: Create a data frame that stores the results, then investigate the `kable()` function from the `knitr` package.
```{r}
library(forecast)#use forecasting library to obtain accuracy function..
library(knitr)
q5_model1 = lm(Ozone$ozone ~ Ozone$wind)
q5_model2 = lm(Ozone$ozone ~ Ozone$humidity)
q5_model3 = lm(Ozone$ozone ~ Ozone$temp)


results = data.frame(model1 = c(model_rmse = data.frame(accuracy(q5_model1))$RMSE, model_r2 = summary(q5_model1)$r.squared),
                     model2 = c(model2rmse = data.frame(accuracy(q5_model2))$RMSE, model2r2 = summary(q5_model2)$r.squared),
                     model3 = c(model3rmse = data.frame(accuracy(q5_model3))$RMSE, model3r2 = summary(q5_model3)$r.squared)
)

kable(head(t(results)))
```



**(b)** Based on the results, which of the three predictors used is most helpful for predicting ozone readings? Briefly explain.

Based on the results, the third model with temperature as the predictor is the most helpful for predicting ozone readings. This model has the lowest Root Mean Square Error, and the highest R-Squared. This model explains the data the best out of all three.

***

## Exercise 00 (SLR without Intercept)

**This exercise will _not_ be graded and is simply provided for your information. No credit will be given for the completion of this exercise. Give it a try now, and be sure to read the solutions later.**

Sometimes it can be reasonable to assume that $\beta_0$ should be 0. That is, the line should pass through the point $(0, 0)$. For example, if a car is traveling 0 miles per hour, its stopping distance should be 0! (Unlike what we saw in the book.)

We can simply define a model without an intercept,

\[
Y_i = \beta x_i + \epsilon_i.
\]

**(a)** [In the **Least Squares Approach** section of the text](http://daviddalpiaz.github.io/appliedstats/simple-linear-regression.html#least-squares-approach) you saw the calculus behind the derivation of the regression estimates, and then we performed the calculation for the `cars` dataset using `R`. Here you need to do, but not show, the derivation for the slope only model. You should then use that derivation of $\hat{\beta}$ to write a function that performs the calculation for the estimate you derived. 

In summary, use the method of least squares to derive an estimate for $\beta$ using data points $(x_i, y_i)$ for $i = 1, 2, \ldots n$. Simply put, find the value of $\beta$ to minimize the function

\[
f(\beta)=\sum_{i=1}^{n}(y_{i}-\beta x_{i})^{2}.
\]

Then, write a function `get_beta_no_int` that takes input:

- `x` - A predictor variable
- `y` - A response variable

The function should then output the $\hat{\beta}$ you derived for a given set of data.

**(b)** Write your derivation in your `.Rmd` file using TeX. Or write your derivation by hand, scan or photograph your work, and insert it into the `.Rmd` as an image. See the [RMarkdown documentation](http://rmarkdown.rstudio.com/) for working with images.

**(c)** Test your function on the `cats` data using body weight as `x` and heart weight as `y`. What is the estimate for $\beta$ for this data?

**(d)** Check your work in `R`. The following syntax can be used to fit a model without an intercept:

```{r, eval = FALSE}
lm(response ~ 0 + predictor, data = dataset)
```

Use this to fit a model to the `cat` data without an intercept. Output the coefficient of the fitted model. It should match your answer to **(c)**.

